---
title: "p8105_hw6_yx2507"
author: "Yuqing Xue"
date: "11/22/2019"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(purrr)
library(modelr)
library(mgcv)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
set.seed(1)
```


# problem 1

## load and clean the data
```{r}
bw_df=read_csv(file = "./data/birthweight.csv")%>%
  janitor::clean_names()%>%
  mutate(
    babysex = fct_infreq(as.factor(babysex)),
    frace = fct_infreq(as.factor(frace)),
    malform = fct_infreq(as.factor(malform)),
    mrace = fct_infreq(as.factor(mrace))
  )

# check missing data
missing= bw_df %>%
   summarise_all((funs(sum(is.na(.))))) 
missing
```

after cleaning the data, we check the missing data and make sure that there is no missing data.

# model fitting"
I hypothesize that baby's birthweight is associated with mother's characteristics like age and weight at delivery. The research hypothesis would be the larger for mother's weight at delivery, the larger the birthweight for the baby. I will fit a linear regression model for this.
```{r}
fit = lm(bwt ~ delwt+ momage, data = bw_df)
  
```

tidy the result 
```{r}
fit %>% 
  broom::tidy() %>% 
  as.tibble()
```

# model diagnosis
```{r}
diagnosis= bw_df%>%
     modelr::add_residuals(fit)%>%
      modelr::add_predictions(fit)

diagnosis_plot=diagnosis%>% 
  ggplot(aes(x= pred, y= resid))+
  geom_point()

diagnosis_plot

```

# compare with more model

1.One using length at birth and gestational age as predictors (main effects only)
```{r}
fit_2=lm(bwt ~ blength + gaweeks, data = bw_df)

fit_2%>%
  broom::tidy() %>% 
  as.tibble()
  
fit_3= lm(bwt ~ bhead + blength + babysex + bhead*blength+bhead*babysex+blength*babysex+ bhead*blength*babysex, data = bw_df)

```
 
# compare the models
```{r}
cv_df= crossv_mc (bw_df, 100)

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))


cv=cv_df%>%
  mutate(fit= map(train,~fit),
         fit_2= map(train,~fit_2),
         fit_3= map(train,~fit_3)
         )%>%
  mutate(rmse_1 = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(fit_2, test, ~rmse(model = .x, data = .y)),
         rmse_3 = map2_dbl(fit_3, test, ~rmse(model = .x, data = .y)))

```

```{r}
plot= cv%>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

plot
```

from the figure, we can tell that model 1 has obviously larger rmse, which means it might not be an appropriate model. Model 1 might lose some important information.


# Promble 2

## read in the data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
## Doing bootstraping 

```{r}
weather=weather_df%>%
  modelr::bootstrap(n= 5000)%>%
  mutate(
    models= map(strap, ~lm(tmax~ tmin, data = .x)),
    results =  map(models, broom::tidy),
    r_square = map(models, broom::glance)
  )%>%
  select(-models, -strap)%>%
  unnest(results, r_square)%>%
  janitor::clean_names()
  

```


## estimate1-- r^2
```{r}
weather %>%
  ggplot(aes(x = r_squared)) +
  geom_density() +
labs(title = "The distribution of R Squared",
       x = "R Squared")
quantile(pull(weather, r_squared),c(0.025,0.975))
```

the figure above shows the distribution of r^2 in the bootstrap dataset tend to be a normal distribution, we can see the 95% confidence interval is (0.8936977, 0.9274955 )

## estimate--`log(beta0*beta1)`

```{r}

log_beta= weather%>%
  select(id, term, estimate)%>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  )%>%
  mutate(intercept=`(Intercept)`,
         log_beta=log(intercept*tmin)
         )

plot_beta=log_beta%>%
  ggplot(aes(x=log_beta))+geom_density()

plot_beta
```

we can see from the plot above, we can tell that the distribution of the `log(beta0*beta1)` is approximately normal distribution. This indicates that bootstrap sampling method is similar to drawing sample from the population and fitting model with that samples. This will result in the parameter estimates from the regression model tend to follow normal distribution.